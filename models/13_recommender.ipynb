{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNE3ufiH0/dvZnBPQ28fcag"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"gEIvjcsSvAcO"}},{"cell_type":"markdown","source":["# Content-Based Recommender"],"metadata":{"id":"UBHO006Ysz8H"}},{"cell_type":"markdown","source":["准备每部电影的 特征向量 x^(i)"],"metadata":{"id":"DzWudeP9s8wY"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"oWhya6LzudAA","executionInfo":{"status":"ok","timestamp":1743600733484,"user_tz":240,"elapsed":3,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"outputs":[],"source":["import numpy as np\n","\n","# 假设我们有3部电影，每部电影的特征是 [浪漫, 动作, 科幻]\n","X = np.array([\n","    [0.8, 0.1, 0.1],  # Movie A\n","    [0.0, 1.0, 0.0],  # Movie B\n","    [0.2, 0.2, 0.6]   # Movie C\n","])\n"]},{"cell_type":"markdown","source":["为每个用户准备一个 兴趣向量 θ^(j)"],"metadata":{"id":"mepqjx1os_0R"}},{"cell_type":"code","source":["theta_alice = np.array([5.0, 0.0, 0.0])  # 她只喜欢浪漫片\n"],"metadata":{"id":"8HMVkK57s9j_","executionInfo":{"status":"ok","timestamp":1743600753624,"user_tz":240,"elapsed":11,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["用点积预测评分 y^(i,j) = (θ^(j))^T x^(i)"],"metadata":{"id":"u76G6hEGtECg"}},{"cell_type":"code","source":["predicted_ratings = X @ theta_alice  # 点积操作\n","print(predicted_ratings)  # 输出这三部电影的预测评分\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZ_QcSa8tEi8","executionInfo":{"status":"ok","timestamp":1743601228620,"user_tz":240,"elapsed":9,"user":{"displayName":"Tao He","userId":"08580683798506804735"}},"outputId":"ce6196b0-2f8a-478c-9de7-8612b5dad0cf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[4. 0. 1.]\n"]}]},{"cell_type":"markdown","source":["要“学的是 θ”，而不是“假设已有 θ”\n","\n","用已有评分 y 来优化 θ，使得 (θ^T x) 尽可能接近 y\n","\n","实现损失函数 J(θ)\n","\n","推出梯度 ∇J(θ)\n","\n","编写 theta 的迭代更新代码"],"metadata":{"id":"rY_0Qw0XvFHv"}},{"cell_type":"code","source":["theta_alice = np.random.randn(3)\n","y = np.array([4.0, 1.0, 2.0])  # Alice 对三部电影的真实评分\n"],"metadata":{"id":"VF0AkwIEvEL6","executionInfo":{"status":"ok","timestamp":1743601975697,"user_tz":240,"elapsed":33,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def loss_function(theta, X, y):\n","    predictions = X @ theta\n","    return np.mean((predictions - y) ** 2)"],"metadata":{"id":"Q1-WMGWCw1eh","executionInfo":{"status":"ok","timestamp":1743601976541,"user_tz":240,"elapsed":2,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def gradient(theta, X, y):\n","    predictions = X @ theta\n","    errors = predictions - y\n","    gradient = X.T @ errors / len(y)\n","    return gradient"],"metadata":{"id":"mPMm4Okmw7-w","executionInfo":{"status":"ok","timestamp":1743601977259,"user_tz":240,"elapsed":1,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["step = 100\n","learning_rate = 0.01\n","for i in range(step):\n","    gradient_theta = gradient(theta_alice, X, y)\n","    theta_alice -= learning_rate * gradient_theta\n","\n","    # 打印损失(每十步)\n","    if i % 10 == 0:\n","      current_loss = loss_function(theta_alice, X, y)\n","      print(f\"Step {i}, Loss: {current_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMrlUOh9xB_f","executionInfo":{"status":"ok","timestamp":1743602001932,"user_tz":240,"elapsed":4,"user":{"displayName":"Tao He","userId":"08580683798506804735"}},"outputId":"d94c656a-58d9-4172-f998-7b455e83f0e9"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 0, Loss: 12.0655\n","Step 10, Loss: 11.3324\n","Step 20, Loss: 10.6462\n","Step 30, Loss: 10.0038\n","Step 40, Loss: 9.4023\n","Step 50, Loss: 8.8390\n","Step 60, Loss: 8.3113\n","Step 70, Loss: 7.8169\n","Step 80, Loss: 7.3537\n","Step 90, Loss: 6.9194\n"]}]},{"cell_type":"markdown","source":["从原来一个人对若干电影的评分数据，扩展成一个**用户-评分矩阵**结构。\n","关键设计变成了：\n","\n","1. 输入结构：\n","\n","  - X：每部电影的特征（不变）\n","  - Y：每个用户对每部电影的评分矩阵（比如 shape 为 m × n）\n","  - R：一个同样 shape 的 0/1 矩阵，表示用户是否给这部电影打过分（mask）\n","\n","2. 参数：\n","\n","  每个用户都有一个 θ^(j)，所以我们现在需要一个 theta_all，shape 是 n × k（n个用户，k个特征）\n","\n","3. 训练：\n","\n","  - 遍历每个用户 j：\n","\n","  - 取出 j 用户有评分的电影行\n","\n","  - 用这些电影的特征和评分训练 θ^(j)\n","\n","  - 存入 theta_all[j]"],"metadata":{"id":"KnAYn27DzAz-"}},{"cell_type":"markdown","source":["准备数据结构（多人版）\n","\n","备注: 掩码矩阵\n","```\n","array([\n","    [False, False, False],\n","    [False,  True, False]\n","])\n","```\n","再取反:\n","```\n","array([\n","    [ True,  True,  True],\n","    [ True, False,  True]\n","])\n","\n","```"],"metadata":{"id":"Og0d23h_3i8k"}},{"cell_type":"code","source":["# 3部电影，每部特征是 [浪漫, 动作, 科幻]\n","X = np.array([\n","    [0.8, 0.1, 0.1],  # Movie A\n","    [0.0, 1.0, 0.0],  # Movie B\n","    [0.2, 0.2, 0.6]   # Movie C\n","])\n","\n","# 2个用户对3部电影的评分（NaN 表示没评分）\n","Y = np.array([\n","    [4.0, 1.0, 2.0],     # Alice\n","    [5.0, np.nan, 1.0]   # Bob\n","])\n","\n","# 构建 R 掩码矩阵：用户是否对该电影打过分\n","R = ~np.isnan(Y)  # True if rated\n"],"metadata":{"id":"EZ3S4N4J3eOg","executionInfo":{"status":"ok","timestamp":1743603818401,"user_tz":240,"elapsed":10,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["为每个用户训练 θ^(j)\n","我们现在要循环每个用户,选出她打过分的电影, 然后单独训练她的向量"],"metadata":{"id":"z78fQnAH3jip"}},{"cell_type":"code","source":["theta_all = [] # 收集所有兴趣向量\n","\n","# 我们开始对每个用户编号，依次叫他们进来训练。假设有 2 个用户，这个 user_id 就会是 0 和 1。\n","for user_id in range(Y.shape[0]):\n","    # 找出当前用户打过分的电影\n","    idx = R[user_id]\n","    X_user = X[idx]           # 这些电影的特征\n","    y_user = Y[user_id][idx]  # 对应评分\n","\n","    # 初始化 θ^(j)\n","    theta = np.random.randn(X.shape[1])\n","\n","    # 梯度下降训练 θ^(j)\n","    for step in range(100):\n","        pred = X_user @ theta\n","        error = pred - y_user\n","        grad = X_user.T @ error / len(y_user)\n","        theta -= 0.01 * grad\n","\n","    theta_all.append(theta)\n"],"metadata":{"id":"fPOiA0d-3gsR","executionInfo":{"status":"ok","timestamp":1743604579796,"user_tz":240,"elapsed":7,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["把训练好的 theta_all 用来“预测”每个用户对所有电影的评分"],"metadata":{"id":"buQfdObg73B4"}},{"cell_type":"code","source":["import numpy as np\n","\n","theta_all = np.array(theta_all)         # 列表转成矩阵\n","predicted_all = theta_all @ X.T         # 每个用户对所有电影的预测评分\n","\n","print(\"预测评分矩阵：\")\n","print(predicted_all)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56T63m4u72Tz","executionInfo":{"status":"ok","timestamp":1743604662299,"user_tz":240,"elapsed":51,"user":{"displayName":"Tao He","userId":"08580683798506804735"}},"outputId":"8af1dddc-cf09-4102-f517-792d462f9324"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["预测评分矩阵：\n","[[ 0.31989075  1.56290138  0.77025802]\n"," [ 1.64955014 -1.17719668  0.52611255]]\n"]}]},{"cell_type":"markdown","source":["# Collaborative Filtering"],"metadata":{"id":"TnOpxwoF9RNi"}},{"cell_type":"markdown","source":["和 content-based 最大不同是：现在 x^(i) 不是事先定义的特征向量，而是也要通过训练学习"],"metadata":{"id":"mZDtJeQ2-Kon"}},{"cell_type":"markdown","source":["1. 首先做矩阵\n","我们会用这个评分矩阵同时训练：\n","\n","theta_all：每个用户的兴趣向量，shape 是 (n_users × k)\n","\n","x_all：每部电影的隐特征向量，shape 是 (n_movies × k)\n","\n","(k是特征的数量 -- 隐变量的维度)"],"metadata":{"id":"LERvKQXk-oYO"}},{"cell_type":"code","source":["# 超简化的评分矩阵(同上)\n","Y = np.array([\n","    [5, 4, np.nan],  # Alice\n","    [np.nan, 3, 1],  # Bob\n","    [2, np.nan, 4]   # Carol\n","])\n","\n","R = ~np.isnan(Y)  # 是否评分的掩码矩阵\n"],"metadata":{"id":"8h06ubmb9P-_","executionInfo":{"status":"ok","timestamp":1743605352366,"user_tz":240,"elapsed":44,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["2. 全部随机初始化参数\n"],"metadata":{"id":"ko4CCTRL-1B0"}},{"cell_type":"code","source":["n_users, n_movies = Y.shape\n","k = 3  # 隐变量维度\n","\n","theta_all = np.random.randn(n_users, k)\n","x_all = np.random.randn(n_movies, k)\n"],"metadata":{"id":"-p2aGOyt-3RY","executionInfo":{"status":"ok","timestamp":1743605433973,"user_tz":240,"elapsed":38,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["3. 定义损失函数（含正则化）"],"metadata":{"id":"zgkccZxE-_Cx"}},{"cell_type":"code","source":["def loss_function(theta_all, x_all, Y, R, λ):\n","    loss = 0\n","    for i in range(n_users):\n","        for j in range(n_movies):\n","            if R[i][j]:\n","                pred = theta_all[i] @ x_all[j]\n","                loss += (pred - Y[i][j]) ** 2\n","    loss *= 0.5\n","    loss += (λ / 2) * (np.sum(theta_all ** 2) + np.sum(x_all ** 2))\n","    return loss\n"],"metadata":{"id":"43lFE2vU_AH0","executionInfo":{"status":"ok","timestamp":1743605481603,"user_tz":240,"elapsed":36,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["4. 梯度下降 theta 和 x"],"metadata":{"id":"ID--y4HoDKu4"}},{"cell_type":"code","source":["def compute_theta_gradient(i, theta_all, x_all, Y, R, λ):\n","  grad = np.zeros_like(theta_all[i])  # 初始梯度是全零\n","  for j in range(x_all.shape[0]):\n","    if R[i][j]:\n","      pred = theta_all[i] @ x_all[j]\n","\n","      error = pred - Y[i][j]\n","      grad += error * x_all[j]\n","  grad += λ * theta_all[i]\n","  return grad\n"],"metadata":{"id":"MeM0nKr3DM-e","executionInfo":{"status":"ok","timestamp":1743607193611,"user_tz":240,"elapsed":46,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def compute_x_gradient(j, theta_all, x_all, Y, R, λ):\n","    grad = np.zeros_like(x_all[j])\n","    for i in range(theta_all.shape[0]):\n","        if R[i][j]:\n","            pred = theta_all[i] @ x_all[j]\n","            error = pred - Y[i][j]\n","            grad += error * theta_all[i]\n","    grad += λ * x_all[j]\n","    return grad\n"],"metadata":{"id":"R0RvLp_rGgr8","executionInfo":{"status":"ok","timestamp":1743607437195,"user_tz":240,"elapsed":39,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def train(theta_all, x_all, Y, R, λ, learning_rate, steps):\n","    for step in range(steps):\n","        # 更新所有 θ⁽ⁱ⁾\n","        for i in range(theta_all.shape[0]):\n","            grad_theta = compute_theta_gradient(i, theta_all, x_all, Y, R, λ)\n","            theta_all[i] -= learning_rate * grad_theta\n","\n","        # 更新所有 x⁽ʲ⁾\n","        for j in range(x_all.shape[0]):\n","            grad_x = compute_x_gradient(j, theta_all, x_all, Y, R, λ)\n","            x_all[j] -= learning_rate * grad_x\n","\n","        # 每10步打印一次 loss\n","        if step % 10 == 0:\n","            loss = loss_function(theta_all, x_all, Y, R, λ)\n","            print(f\"Step {step} | Loss: {loss:.4f}\")\n"],"metadata":{"id":"55rKoXOpGiX5","executionInfo":{"status":"ok","timestamp":1743607442669,"user_tz":240,"elapsed":9,"user":{"displayName":"Tao He","userId":"08580683798506804735"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["train(theta_all, x_all, Y, R, λ=0.1, learning_rate=0.01, steps=100)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCMZLwKQGkI_","executionInfo":{"status":"ok","timestamp":1743607449894,"user_tz":240,"elapsed":5,"user":{"displayName":"Tao He","userId":"08580683798506804735"}},"outputId":"d1382958-5711-4dc3-b359-401910780cbc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 0 | Loss: 36.9784\n","Step 10 | Loss: 11.9560\n","Step 20 | Loss: 5.2829\n","Step 30 | Loss: 3.8392\n","Step 40 | Loss: 3.3513\n","Step 50 | Loss: 3.0418\n","Step 60 | Loss: 2.7883\n","Step 70 | Loss: 2.5635\n","Step 80 | Loss: 2.3579\n","Step 90 | Loss: 2.1688\n"]}]}]}